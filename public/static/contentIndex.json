{"index":{"title":"Burger Math","links":["math/Tensors"],"tags":[],"content":"\nTopics:\nTensors and Diff-Geo"},"math/Euclidean_spaces":{"title":"Euclidian Geometry and Spaces","links":[],"tags":[],"content":"Before we delve into the algebra of Euclidean vector spaces, it’s essential to understand the geometric foundations upon which this algebra is built. Throughout much of history, mathematical thought was often justified and expressed using geometric relations, as symbolic reasoning wasn’t developed enough to replicate what could be done with even simple straight edge and compass constructions.\n\n  \n"},"math/Linear_combinations":{"title":"Linear Combinations","links":["math/linear_independence"],"tags":[],"content":"The vector axioms define the properties of scalar multiplication and vector addition, but often we’ll seek to make use of both operations at once. This leads us to the more general concept of a linear combination.\nA linear combination of vectors is simply just a sum of vectors weighted by scalar coefficients. Given any indexable set of vectors S\\equiv \\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} within a space \\textbf{V} we can compactly represent any linear combination of vectors within this set like:\n\\begin{gathered}\n\\displaystyle\\sum_{i=1}^{n}c_{i}\\vec{u}_i=c_1\\vec{u}_1+...+c_n\\vec{u}_n\n\\end{gathered}\nWhere each coefficient could be any arbitrary scalar. Also take note that since \\textbf{V} is closed under both addition and multiplication, a linear combination of vectors is itself a vector. There isn’t much added benefit that comes from discussing specific linear combinations of vectors, as they are already fully described by the vector axioms. The main benefit comes from the ability to construct the set of all linear combinations of a given set S, which we call the span.\nFormally the span of a subset S of a vector space \\textbf{V} over a  Field {\\textbf{F}} can be defined as:\n\\begin{gathered}\nspan(S)= \\left\\{\n\\sum_{i=1}^{n}c_{i}\\vec{u}_{i} \\,\\middle|\\, c_{i}\\in\\mathbf{F},\\vec{u}_i\\in S\n\\right\\}\n\\end{gathered}\nThe span of a set of vectors is special as far as subsets of \\textbf{V} go in that it can be demonstrated to have its own vector space structure. All axioms dealing with the properties of operations are automatically met as a subset of a \\textbf{V}, and the remaining two axioms (existence of additive inverse and identity) can be shown to be satisfied in the following ways:\nAdditive Inverse\n\\begin{gathered}\nlet \\,\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\equiv\\vec{v}. \\\\ \\\\\\forall\\vec{v}\\in span(S),\\\\\\\\ -\\vec{v}=-\\left(\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\right) =\\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\,,\\\\\\\\ \\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nAdditive identity\n\\begin{gathered}\n\\vec{0}=\\sum_{i=1}^n\\vec{0}=\\sum_{i=1}^n0\\vec{u}_{i}\\, ,\\\\\\\\\\sum_{i=1}^{n}0\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nThus given any set of vectors in \\textbf{V}, the span of the set is simultaneously a subset of \\textbf{V} and a vector space of its own. All such subsets are referred to as subspaces and are generally assumed to be subsets of larger vector spaces, though the span of a set could be the entire vector space itself (For the trivial example, span(\\{0\\})=\\{0\\}) )\nThe span of a set of vectors is not unique however. For example, take two sets in \\textbf{V}:\n\\begin{gathered}\nS_{1}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_{n}\\} \\text{ and } S_{2}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_n+\\vec{u}_1\\}. \\\\[1em]\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\nspan(S_{2})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n-1}c_{i}\\vec{u}_{i}\\right)+c_n\\left(\\vec{u}_{n}+\\vec{u}_{1}\\right)\\middle|c_{i}, c_{n}\\in\\textbf{F}\\right\\}\n\\end{gathered}\nWe can pretty easily see that the span of both sets should be the same, as we could redefine c_{1}\\equiv c_{1}+c_{n} (since the coefficients c_i   are arbitrary) and construct the span of both sets in exactly the same form.\nSimilarly for a triplet of sets:\n\\begin{gathered}\nS_{1} \\equiv \\{\\vec{u}_{1} \\cdots, \\vec{u}_{n}\\}, \\,\nS_{2} \\equiv \\left\\{\\sum_{i=1}^n b_{i}\\vec{u}_i\\right\\}, \\text{ and }\nS_{3} \\equiv S_{1} \\cup S_{2},\n\\end{gathered}\nbeing a member of the union of two sets means an element is within either set, or tthe intersection of both. A vector being within the span of the union of two sets means the vector is a linear combination of members of either set, or both sets. Thus we can represent the spans of S_{1} and the union S_{1}\\cup S_{2} like\n\\begin{gathered}\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\\\\\nspan(S_{3})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n}a_{i}\\vec{u}_{i}\\right)+c\\left(\\displaystyle\\sum_{j=1}^{n}b_{j}\\vec{u}_{j}\\right)\\middle|a_{i},b_{j},c\\in\\textbf{F}\\right\\}.\n\\end{gathered}\nLike we had in the previous example we can combine both sums in the definition of span(S_{3}) by redefining a_{i}+c(b_{i})\\equiv c_{i}. Doing so reduces the two sums into one and we can see both S_{1} and S_{3} have the same span.\nThe first and second examples illustrate an important concept: the subspace spanned by a set of vectors is not unique. However, there is a crucial distinction between the two scenarios:\n\nIn the first example, we have two sets with the same number of elements that span the same subspace. This demonstrates that different sets can generate the same span.\nThe second example involves adding a new element to an existing set without changing the span. This shows that adding certain elements to a set may not expand the subspace it generates.\n\nThis distinction leads us to a fundamental property of sets of vectors known as linear independence."},"math/Poopy":{"title":"Vector Bases and Coordinate Vectors","links":[],"tags":[],"content":"Building upon our understanding of vector spaces, linear combinations, span, and linear independence, we now turn to one of the most powerful concepts in linear algebra: the vector basis.\nA basis B_{W} for a subspace \\textbf{W} is a set of vectors that is both linearly independent and spanning for \\textbf{W}. A vector basis may be defined for any subspace and is not limited to  propersubspaces. In fact any set with vector space structure has at minimum one basis. Typically denoted as \\{\\vec{e}_{1},\\cdots,\\vec{e}_{n}\\}, a basis has a crucial property: it’s minimal. This means that removing any vector from the basis would result in a set that no longer spans \\textbf{W}.\nThis minimality property leads us to a fundamental characteristic of vector spaces: dimension. The dimension of a vector space is defined as the number of vectors in any basis for that space, and is an intrinsic property held by all spaces.\nDespite the abstract presentation of these (hopefully familiar) concepts, bases and dimension are two of the most practical ideas in linear algebra due to how they simplify and give meaning to abstract vector operations. Taking advantage of the fact that any vector in a space \\textbf{V} is within the span of a basis B_{V}\\equiv \\{ \\vec{e}_{1},\\cdots, \\vec{e}_{n}\\}, we can express all vectors \\vec{u}\\in\\textbf{V} as linear combinations\n\\begin{gathered}\n\\vec{u} = \\sum_{i=1}^n u_{i}\\vec{e}_{i}\n\\end{gathered}\nwhere each u_i is called a component or coordinate of \\vec{u} with respect to the basis B_{V}. The utility of this expression is immediately obvious when we try to make sense of the sum of two vectors. Take for example the vector equation \\vec{u} + \\vec{v} = \\vec{w}. Without a basis, this equation provides limited information and is little more than a definition. However, when we express these vectors in terms of a basis, the equation becomes:\n\\begin{gathered}\n\\vec{u}+ \\vec{v} = \\left(\\sum_{i=1}^{n} u_{i} \\vec{e}_{i}\\right) + \\left(\\sum_{j=1}^{n} v_{j} \\vec{e}_{j}\\right) = \\sum_{i=1}^{n} (u_{i}+v_{i}) \\vec{e}_{i} = \\sum_{i=1}^{n} w_{i} \\vec{e}_{i} = \\vec{w} \n\\end{gathered}\nThis representation allows us to compute the components of \\vec{w} directly from those of \\vec{u} and \\vec{v}, and to identify relationships between vectors that might not be apparent in their abstract form.\nFor instance, consider the case where \\vec{u} = \\vec{e}_{1} + \\vec{e}_{2} and \\vec{v} = 2\\vec{e}_{1} + 2\\vec{e}_{2} for the previous equation. In this basis representation, we can immediately see that \\vec{v} = 2\\vec{u} (and thus \\vec{w} = 3 \\vec{u} ).\nThis"},"math/Tensors":{"title":"Tensors","links":["math/Vectors","math/Euclidean_spaces"],"tags":[],"content":"\nSub-Topics\nAbstract vector space review\nEuclidean vector spaces"},"math/Vector_Bases":{"title":"Vector Bases and Coordinate Vectors","links":["math/Euclidean_spaces"],"tags":[],"content":"Building upon our understanding of vector spaces, linear combinations, span, and linear independence, we now turn to one of the most fundamental concepts in linear algebra: the vector basis. In this section, we’ll define what a basis is, explore its properties, and see how bases allow us to represent vectors numerically.\nA basis B_{W} for a subspace \\textbf{W} is a set of vectors that is both linearly independent and spanning for \\textbf{W}. A vector basis may be defined for any subspace and is not limited to  proper subspaces. In fact, any set with vector space structure has at least one basis. Typically denoted as \\{\\vec{e}_{1},\\cdots,\\vec{e}_{n}\\}, a basis has a crucial property: it’s minimal. This means that removing any vector from the basis would result in a set that no longer spans \\textbf{W}.\nThis minimality property leads us to a fundamental characteristic of vector spaces: dimension. The dimension of a vector space is defined as the number of vectors in any basis for that space. Importantly, all bases for a given space have the same number of vectors, so dimension is an intrinsic property of the space itself.\nDespite their abstract nature, bases and dimension are two of the most practical ideas in linear algebra, as they simplify and give meaning to abstract vector operations. The power of these concepts lies in their ability to represent any vector in a space using a finite set of numbers.\nSince a basis for a space \\textbf{V} spans the entire space, any vector in \\textbf{V} is within the span and may be expressed as a linear combination of the basis vectors, which is guaranteed to be unique. Specifically, given a basis B_{V}\\equiv \\{ \\vec{e}_{1},\\cdots, \\vec{e}_{n}\\}, we can express all vectors \\vec{u}\\in\\textbf{V} as:\n\\begin{gathered}\n\\vec{u} = \\sum_{i=1}^n u_{i}\\vec{e}_{i}\n\\end{gathered}\nwhere each u_i is called a component or coordinates of \\vec{u} with respect to the basis B_{V}. Importantly, the number of these coordinates is always equal to the dimension of the space.\nThe utility of this expression is immediately obvious when we try to make sense of the sum of two vectors. Take for example the vector equation \\vec{u} + \\vec{v} = \\vec{w}. Without a basis, this equation provides limited information and is little more than a definition. However, when we express these vectors in terms of a basis, the equation becomes:\n\\begin{gathered}\n\\vec{u}+ \\vec{v} = \\left(\\sum_{i=1}^{n} u_{i} \\vec{e}_{i}\\right) + \\left(\\sum_{j=1}^{n} v_{j} \\vec{e}_{j}\\right) = \\sum_{i=1}^{n} (u_{i}+v_{i}) \\vec{e}_{i} = \\sum_{i=1}^{n} w_{i} \\vec{e}_{i} = \\vec{w}\n\\end{gathered}\nThis representation allows us to compute the components of \\vec{w} directly from those of \\vec{u} and \\vec{v}, and to identify relationships between vectors that might not be apparent in their abstract form.\nFor instance, consider the case where \\vec{u} = \\vec{e}_{1} + \\vec{e}_{2} and \\vec{v} = 2\\vec{e}_{1} + 2\\vec{e}_{2} for the previous equation. Despite having never defined \\vec{v} in terms of \\vec{u}, by looking at the coordinates of the vectors we can immediately see that \\vec{v} = 2\\vec{u} (and thus \\vec{w} = 3 \\vec{u} ).\n\nCoordinate Vectors\nThe utility of basis representations is so widely recognized that it’s common to equate the coordinates of a vector relative to a basis with the vector itself. While they are technically distinct, we can establish a rigorous mathematical relationship between a vector and its coordinates that justifies this intuition.\nIn a finite n-dimensional vector space \\textbf{V} with a basis B_{V} \\equiv \\{\\vec{e}_1, \\cdots, \\vec{e}_n\\}, we already know we can express a vector \\vec{u} as\n\\begin{gathered}\n\\sum_{i=1}^{n} u_{i} \\vec{e}_{i}\n\\end{gathered}\nHowever, if we make the choice to perform all calculations in a specified basis, we don’t need to explicitly show the basis vectors. Instead, it can be understood that for a vector \\vec{u}, the coordinate u_{i} corresponds to \\vec{e}_{i}, and we could represent \\vec{u} with the ordered n-tuple (u_{1},\\cdots,u_{n}) or column matrix:\n\\begin{bmatrix}\nu_{1}\\\\\n\\vdots \\\\\nu_{n}\n\\end{bmatrix}\ninstead. This n-tuple (or column matrix) is known as the coordinate-vector representation of \\vec{u} in a given basis, and is actually a member of its own distinct vector space over the same field \\textbf{F} as \\textbf{V}, known as \\textbf{F}^n (\\textbf{R}^n or \\textbf{C}^n usually).\nWhen we say that there is a rigorous justification for the intuition that  \\textbf{V} and \\textbf{F}^n are the same, we aren’t claiming that the two are equal as sets, but that we can uniquely identify members of one with the other, but rather that the there is an isomorphism between the two.\nAn isomorphism is a way of saying that two mathematical objects have the same structure, even if they appear different on the surface. In the context of vector spaces, two spaces are isomorphic if there’s a way to transform vectors from one space to the other while preserving all vector operations (like addition and scalar multiplication).\nMore formally, in linear algebra, if there exists a bijective linear map f: \\textbf{V} \\to \\textbf{W} (that is, a one-to-one and onto linear transformation), then f is an isomorphism and we say \\textbf{V} \\cong \\textbf{W} (the two are isomorphic). A key result of this is that two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.\nIn the case of an n-dimensional space \\textbf{V}, we can define a map \\phi_{B_{V}} that takes a vector \\vec{u} \\in \\textbf{V} to its coordinate representation in a basis B_{V}:\n\\begin{gathered}\n\\phi_{B_{V}} : \\textbf{V} \\to \\textbf{F}^n \\,; \\vec{u} \\mapsto \n\\begin{bmatrix}\nu_{1}\\\\\n\\vdots \\\\\nu_{n}\n\\end{bmatrix}\n\\end{gathered}\nTrivially both spaces are n dimensional, and \\phi_{B_{V}} must be bijective as every vector has a unique coordinate representation (injectivity) and every coordinate representation corresponds to some vector (surjectivity)\nThus \\phi_{B_{V}} is an isomorphism between the two spaces. This isomorphism is the foundation for the entirety of most first courses in linear algebra, as matrix algebra is only possible due to the ability to reason about abstract vectors using their representations as lists of numbers.\nWhile there is much more that could be said about vectors and vector spaces from an abstract point of view, countless excellent resources already explore this area of linear algebra,  Rather than delve further into abstract generalities, we save some ideas for later when they will become more relevant and shift our focus to perhaps the least abstract class of vector space: Euclidean vector spaces."},"math/Vector_Bases_bk":{"title":"Vector Bases and Coordinate Vectors","links":[],"tags":[],"content":"Building upon our understanding of vector spaces, linear combinations, span, and linear independence, we now turn to one of the most powerful concepts in linear algebra: the vector basis.\nA basis B_{W} for a subspace \\textbf{W} is a set of vectors that is both linearly independent and spanning for \\textbf{W}. A vector basis may be defined for any subspace and is not limited to  propersubspaces. In fact any set with vector space structure has at minimum one basis. Typically denoted as \\{\\vec{e}_{1},\\cdots,\\vec{e}_{n}\\}, a basis has a crucial property: it’s minimal. This means that removing any vector from the basis would result in a set that no longer spans \\textbf{W}.\nThis minimality property leads us to a fundamental characteristic of vector spaces: dimension. The dimension of a vector space is defined as the number of vectors in any basis for that space, and is an intrinsic property held by all spaces.\nDespite the abstract presentation of these (hopefully familiar) concepts, bases and dimension are two of the most practical ideas in linear algebra due to how they simplify and give meaning to abstract vector operations. Taking advantage of the fact that any vector in a space \\textbf{V} is within the span of a basis B_{V}\\equiv \\{ \\vec{e}_{1},\\cdots, \\vec{e}_{n}\\}, we can express all vectors \\vec{u}\\in\\textbf{V} as linear combinations\n\\begin{gathered}\n\\vec{u} = \\sum_{i=1}^n u_{i}\\vec{e}_{i}\n\\end{gathered}\nwhere each u_i is called a component or coordinate of \\vec{u} with respect to the basis B_{V}. The utility of this expression is immediately obvious when we try to make sense of the sum of two vectors. Take for example the vector equation \\vec{u} + \\vec{v} = \\vec{w}. Without a basis, this equation provides limited information and is little more than a definition. However, when we express these vectors in terms of a basis, the equation becomes:\n\\begin{gathered}\n\\vec{u}+ \\vec{v} = \\left(\\sum_{i=1}^{n} u_{i} \\vec{e}_{i}\\right) + \\left(\\sum_{j=1}^{n} v_{j} \\vec{e}_{j}\\right) = \\sum_{i=1}^{n} (u_{i}+v_{i}) \\vec{e}_{i} = \\sum_{i=1}^{n} w_{i} \\vec{e}_{i} = \\vec{w} \n\\end{gathered}\nThis representation allows us to compute the components of \\vec{w} directly from those of \\vec{u} and \\vec{v}, and to identify relationships between vectors that might not be apparent in their abstract form.\nFor instance, consider the case where \\vec{u} = \\vec{e}_{1} + \\vec{e}_{2} and \\vec{v} = 2\\vec{e}_{1} + 2\\vec{e}_{2} for the previous equation. In this basis representation, we can immediately see that \\vec{v} = 2\\vec{u} (and thus \\vec{w} = 3 \\vec{u} ).\nThis"},"math/Vectors":{"title":"Vectors","links":["math/Euclidean_spaces","math/What_is_a_vector"],"tags":[],"content":"\nWhy talk about vectors???\nGiven the complexity of some of the topics that will be covered in this guide, it may seem odd to start with what is likely the billionth vector review for most. While annoying for some, you can rest easy knowing this section is not intended to be a comprehensive lesson on linear-algebra, and in fact will seem blazingly quick for someone with little to no prior knowledge about vector spaces (if you have no idea what a vector is, you might find this series helpful).\nInstead, this review of some basic concepts in linear algebra serves two purposes:\n\n\n1.) Disambiguate notation that will be commonly used throughout this website and introduce readers to a more abstract formulation than the matrix algebra they may be used to\n\n\n2.) Provide a smooth transition into the discussion of Euclidean vector spaces and tensors\n\n\nAfter this review of abstract vector spaces, it will be assumed that you know at least the basics of linear algebra and the properties of vector spaces.\nIf you feel super confident in your knowledge and feel the first section would be a waste of time, click here.\nIf you want to go ahead with the vector review, then the first question we should ask ourselves is, What is a vector?"},"math/What_is_a_vector":{"title":"What is a Vector?","links":["math/Linear_combinations"],"tags":[],"content":"The most concise answer is that a vector \\vec {u} is an element of a vector space \\textbf{V}. In order for a set \\textbf{V} to constitute a vector space, we require the existence of two operations on elements of \\textbf{V} known as vector addition and scalar multiplication. These operations must be defined in a way which is compatible with a set of requirements (called the vector axioms) that define the structure of a vector space. The vector axioms can be expressed in the following concise form:\n\n\\begin{gathered}\n\\forall \\{\\vec{u},\\vec{v},\\vec{w}\\}\\in\\textbf{V},\n\\end{gathered}\n\n\n1.)\\quad\\vec{u}+\\vec{v}\\in\\textbf{V},\\quad a\\vec{u}\\in\\textbf{V}   |   (Closure under vector addition and scalar multiplication)\n\n\n2.)\\quad\\vec{u}+(\\vec{v}+\\vec{w})=(\\vec{u}+\\vec{v})+\\vec{w}   |   (associativity of addition)\n\n\n3.)\\quad\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}   |   (commutativity of addition)\n\n\n4.)\\quad\\exists!{\\vec{0}}\\in\\textbf{V} \\,s.t. \\quad\\vec{0}+\\vec{u}=\\vec{u}   |   (existence of additive identity)\n\n\n5.)\\quad\\exists!(−{\\vec{u}})\\in\\textbf{V} \\,s.t. \\quad(−\\vec{u})+\\vec{u}=\\vec{0}   |   (existence of additive inverse)\n\n\n6.)\\quad a(b\\vec{u})=(ab)\\vec{u}   |   (scalar multiplication is equivalent to Field multiplcation)\n\n\n7.)\\quad1\\vec{u}=\\vec{u}   |   (multiplication by the identity element returns the vector original vector)\n\n\n8.)\\quad(a+b)(\\vec{u}+\\vec{v})=(a+b)\\vec{u}+(a+b)\\vec{v}=a(\\vec{u}+\\vec{v})+b(\\vec{u}+\\vec{v})=a\\vec{u}+b\\vec{u}+a\\vec{v}+b\\vec{v}   |   (scalar multiplication distributes)\n\n\n(\\forall  means for all, \\exists! means there exists only one, and \\in means within)\n\nThis definition is rather dense and includes sets not commonly thought of as vector spaces (such as the set of nth degree polynomials or even \\{0\\}). Luckily most of these requirements are things we tend to take for granted (like associativity) and so we wont have to reference them too much. We’ll also mostly be focusing on Euclidean vector spaces and that comes with a lot of geometric intuition that we can leverage.\nThe vector axioms define how to add vectors and multiply them by scalars, but a general framework for how to contruct new vectors by combining both of these operations is going to be helpful moving forward. Thus we must introduce the concept of a Linear Combination."},"math/a":{"title":"Vector Bases and Coordinate Vectors","links":[],"tags":[],"content":"Building upon our understanding of vector spaces, linear combinations, span, and linear independence, we now turn to one of the most powerful concepts in linear algebra: the vector basis.\nA basis B_{W} for a subspace \\textbf{W} is a set of vectors that is both linearly independent and spanning for \\textbf{W}. A vector basis may be defined for any subspace and is not limited to  propersubspaces. In fact any set with vector space structure has at minimum one basis. Typically denoted as \\{\\vec{e}_{1},\\cdots,\\vec{e}_{n}\\}, a basis has a crucial property: it’s minimal. This means that removing any vector from the basis would result in a set that no longer spans \\textbf{W}.\nThis minimality property leads us to a fundamental characteristic of vector spaces: dimension. The dimension of a vector space is defined as the number of vectors in any basis for that space, and is an intrinsic property held by all spaces.\nDespite their abstract nature, bases and dimension are two of the most practical ideas in linear algebra, as they simplify and give meaning to abstract vector operations. The power of these concepts lies in their ability to represent any vector in a space using a finite set of numbers.\nSince a basis for a space \\textbf{V} spans the entire space, any vector in \\textbf{V} is within the span and may be expressed as a linear combination of the basis vectors, which is guaranteed to be unique. Specifically, given a basis B_{V}\\equiv \\{ \\vec{e}_{1},\\cdots, \\vec{e}_{n}\\}, we can express all vectors \\vec{u}\\in\\textbf{V} as:\n\\begin{gathered}\n\\vec{u} = \\sum_{i=1}^n u_{i}\\vec{e}_{i}\n\\end{gathered}\nwhere each u_i is called a component or coordinate of \\vec{u} with respect to the basis B_{V}. Importantly, the number of these coordinates is always equal to the dimension of the space.\nThe utility of this expression is immediately obvious when we try to make sense of the sum of two vectors. Take for example the vector equation \\vec{u} + \\vec{v} = \\vec{w}. Without a basis, this equation provides limited information and is little more than a definition. However, when we express these vectors in terms of a basis, the equation becomes:\n\\begin{gathered}\n\\vec{u}+ \\vec{v} = \\left(\\sum_{i=1}^{n} u_{i} \\vec{e}_{i}\\right) + \\left(\\sum_{j=1}^{n} v_{j} \\vec{e}_{j}\\right) = \\sum_{i=1}^{n} (u_{i}+v_{i}) \\vec{e}_{i} = \\sum_{i=1}^{n} w_{i} \\vec{e}_{i} = \\vec{w} \n\\end{gathered}\nThis representation allows us to compute the components of \\vec{w} directly from those of \\vec{u} and \\vec{v}, and to identify relationships between vectors that might not be apparent in their abstract form.\nFor instance, consider the case where \\vec{u} = \\vec{e}_{1} + \\vec{e}_{2} and \\vec{v} = 2\\vec{e}_{1} + 2\\vec{e}_{2} for the previous equation. Despite having never defined \\vec{v} in terms of \\vec{u}, by looking at the coordinates of the vectors we can immediately see that \\vec{v} = 2\\vec{u} (and thus \\vec{w} = 3 \\vec{u} ).\n\nThe utility of basis representations is so widely recognized that many people often equate the coordinates on a basis with the vector itself. While they are technically distinct, if we fix a basis for a vector space \\textbf{V}, we can establish a rigorous mathematical relationship between a vector and its coordinates that justifies this intuition.\nIn a finite n-dimensional vector space \\textbf{V} with a basis B_{V} \\equiv \\{\\vec{e}_1, \\cdots, \\vec{e}_n\\}, we already know we can express a vector \\vec{u} as\n\\begin{gathered}\n\\sum_{i=1}^{n} u_{i} \\vec{e}_{i} \n\\end{gathered}\nHowever if we make the choice to perform all calculations in a specified basis, we don’t need to explicitly show the basis vectors. Instead it can be understood that for a vector \\vec{u}, the coordinate u_{i} corresponds to \\vec{e}_{i}  and we could represent \\vec{u} with the ordered n-tuple (u_{1}.\\cdots,u_{n}) or column matrix:\n\\begin{bmatrix}\nu_{1}\\\\\n\\vdots \\\\\nu_{n}\n\\end{bmatrix} \ninstead. This n-tuple (or column matrix) is known as the coordinate-vector representation of \\vec{u} in a given basis, and is actually a member of it’s own distinct vector space over the same field \\textbf{F} as \\textbf{V} known as \\textbf{F}^n (\\textbf{R}^n or \\textbf{C}^n usually).\nWhen we say that there is rigorous justification for the intuition that the coordinate vector space \\textbf{F}^n and \\textbf{V} are essentially the same, we’re referring to a fundamental concept in linear algebra: isomorphism.\nAn isomorphism is a way of saying that two mathematical objects have the same structure, even if they appear different on the surface. In the context of vector spaces, two spaces are isomorphic if there’s a way to transform vectors from one space to the other while preserving all vector operations (like addition and scalar multiplication).\nMore formally, in linear algebra, if there exists a bijective linear map f: \\textbf{V} \\to \\textbf{W} (that is, a one-to-one and onto linear transformation), then f is an isomorphism and we say \\textbf{V} \\cong \\textbf{W} (the two are isomorphic). A key fact is that two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.\nIn our case, the map that takes a vector in \\textbf{V} to its coordinate representation in \\textbf{F}^n is indeed an isomorphism. This isomorphism allows us to transition seamlessly between the abstract vector space \\textbf{V} and the concrete numerical computations in \\textbf{F}^n, justifying our intuition about working with coordinate vectors."},"math/linear_independence":{"title":"Linear Independence","links":["math/Euclidean_spaces","math/Vector_Bases"],"tags":[],"content":"A linearly independent set A\\equiv\\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} has the property that no element in A is a linear combination of any other elements (or lies within the span of the other elements). Symbolically this means\n\\begin{gathered}\n\\forall \\vec{u}_{i}\\in A,\\,\\, \\vec{u}_{i}\\notin span(A\\backslash\\{\\vec{u}_{i}\\}).\n\\end{gathered}\nUnrusprisingly, the complement to linear independence is known as linear dependence. A linearly dependent set of vectors contains elements which can be expressed as linear combinations of other elements in the set. Its important to note that no single element of a linearly dependent set is dependent, but rather they all are. This is because if we have a dependent set  B\\equiv\\{\\vec{v}_1,\\cdots,\\vec{v}_n\\} and an equation expressing any one vector in B like\n\\begin{gathered}\n\\vec{v}_{i}=\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\,\n\\end{gathered}\nthen we can express some other vector in B as a linear combinations of the other elements by first subtracting \\vec{v}_{i} from both sides to get the zero vector\n\\begin{gathered}\n\\vec{0}= \\left(\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\right) -\\vec{v}_{i}\n\\end{gathered}\ndefining c_{i}\\equiv -1 to include -\\vec{v}_i in the sum 1\n\\begin{gathered}\n\\vec{0}= \\sum_{j=1}^{n}c_{j}\\vec{v}_{j}\n\\end{gathered}\nsubtracting from both sides some c_{k}\\vec{v}_k (with c_{k}\\ne 0)\n\\begin{gathered}\n(-c_{k})\\vec{v}_k = \\sum_{j\\neq{k}}^{n} c_j \\vec{v}_j\n\\end{gathered}\nand finally multiplying both sides by -\\frac{1}{c_{k}} to get\n\\begin{gathered}\n\\vec{v}_k = \\left(-\\frac{1}{c_k}\\right)\\sum_{j \\neq k}^{n} c_j \\vec{v}_j =\\sum_{j\\neq{k}}^n \\left(-\\frac{c_{j}}{c_{k}}\\right)\\vec{v}_j\n\\end{gathered}\nwhich is an equation expressing \\vec{v}_k in terms of a linear combination of other vectors in B, making \\vec{v}_k also a dependent element.\n\nOne important fact regarding span and linear independence is that if you consider all possible spanning sets for a given subspace \\textbf{W} and focus on the ones with the smallest number of vectors, you’ll discover that each of these minimal spanning is linearly independent, and that all linearly independent spanning sets are minimal.\nWhile we won’t prove this claim outright here, we can provide some justification by examining the implications of the existence of a set S\\equiv\\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} that is both a minimal spanning set and linearly dependent.\nIf S were dependent, then we could express any one of the elements \\vec{u}_i as a linear combination of the others, as follows:\n\\begin{gathered}\n\\vec{u}_i = \\sum_{j\\neq {i}}^{n} c_{j}\\vec{v}_{j}\\,\\,.\n\\end{gathered}\nRemoving the element \\vec{u}_i from S would not alter the span of the set since any vector within span(S) that is expressed in terms of \\vec{u}_i could instead be expressed using the other vectors in the set.\nThe set S\\backslash\\{\\vec{u_{i}}\\} (which is just S excluding \\vec{u}_i) is smaller than S but has the same span, and thus S could not have been a minimal spanning set.\nSince we can repeat this process until we are left with a linearly independent set, all minimal spanning sets must be linearly independent.\nThis relationship between an independent spanning set and the space it spans brings us to one of the most important and powerful constructions in linear algebra (which will also be the last two topic we’ll cover before getting to Euclidean vector spaces), basis vectors and coordinate spaces.\nFootnotes\n\n\nAs an aside, this line provides us an alternative definition of linear dependence, which is that for any linearly dependent set \\\\B\\equiv\\{\\vec{v}_1,\\cdots,\\vec{v}_n\\},\\,\\,\\exists \\displaystyle\\sum_{i=1}^n c_{i}\\vec{v}_{i} with not all c_{i}=0 \\text{ s.t. } \\displaystyle\\sum_{i=1}^n c_{i}\\vec{v}_{i} = \\vec{0} ↩\n\n\n"}}