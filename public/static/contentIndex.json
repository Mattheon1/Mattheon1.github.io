{"index":{"title":"Burger Math","links":["math/Tensors"],"tags":[],"content":"\nTopics:\nTensors and Diff-Geo"},"math/Euclidean_spaces":{"title":"Euclidean_spaces","links":[],"tags":[],"content":""},"math/Linear_combinations":{"title":"Linear Combinations","links":["math/linear_independence"],"tags":[],"content":"The vector axioms define the properties of scalar multiplication and vector addition, but often we’ll seek to make use of both operations at once. This leads us to the more general concept of a linear combination.\nA linear combination of vectors is simply just a sum of vectors weighted by scalar coefficients. Given any indexable set of vectors S\\equiv \\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} within a space \\textbf{V} we can compactly represent any linear combination of vectors within this set like:\n\\begin{gathered}\n\\displaystyle\\sum_{i=1}^{n}c_{i}\\vec{u}_i=c_1\\vec{u}_1+...+c_n\\vec{u}_n\n\\end{gathered}\nWhere each coefficient could be any arbitrary scalar. Also take note that since \\textbf{V} is closed under both addition and multiplication, a linear combination of vectors is itself a vector. There isn’t much added benefit that comes from discussing specific linear combinations of vectors, as they are already fully described by the vector axioms. The main benefit comes from the ability to construct the set of all linear combinations of a given set S, which we call the span.\nFormally the span of a subset S of a vector space \\textbf{V} over a  Field {\\textbf{F}} can be defined as:\n\\begin{gathered}\nspan(S)= \\left\\{\n\\sum_{i=1}^{n}c_{i}\\vec{u}_{i} \\,\\middle|\\, c_{i}\\in\\mathbf{F},\\vec{u}_i\\in S\n\\right\\}\n\\end{gathered}\nThe span of a set of vectors is special as far as subsets of \\textbf{V} go in that it can be demonstrated to have its own vector space structure. All axioms dealing with the properties of operations are automatically met as a subset of a \\textbf{V}, and the remaining two axioms (existence of additive inverse and identity) can be shown to be satisfied in the following ways:\nAdditive Inverse\n\\begin{gathered}\nlet \\,\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\equiv\\vec{v}. \\\\ \\\\\\forall\\vec{v}\\in span(S),\\\\\\\\ -\\vec{v}=-\\left(\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\right) =\\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\,,\\\\\\\\ \\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nAdditive identity\n\\begin{gathered}\n\\vec{0}=\\sum_{i=1}^n\\vec{0}=\\sum_{i=1}^n0\\vec{u}_{i}\\, ,\\\\\\\\\\sum_{i=1}^{n}0\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nThus given any set of vectors in \\textbf{V}, the span of the set is simultaneously a subset of \\textbf{V} and a vector space of its own. All such subsets are referred to as subspaces and are generally assumed to be subsets of larger vector spaces, though the span of a set could be the entire vector space itself (For the trivial example, span(\\{0\\})=\\{0\\}) )\nThe span of a set of vectors is not unique however. For example, take two sets in \\textbf{V}:\n\\begin{gathered}\nS_{1}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_{n}\\} \\text{ and } S_{2}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_n+\\vec{u}_1\\}. \\\\[1em]\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\nspan(S_{2})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n-1}c_{i}\\vec{u}_{i}\\right)+c_n\\left(\\vec{u}_{n}+\\vec{u}_{1}\\right)\\middle|c_{i}, c_{n}\\in\\textbf{F}\\right\\}\n\\end{gathered}\nWe can pretty easily see that the span of both sets should be the same, as we could redefine c_{1}\\equiv c_{1}+c_{n} (since the coefficients c_i   are arbitrary) and construct the span of both sets in exactly the same form.\nSimilarly for a triplet of sets:\n\\begin{gathered}\nS_{1} \\equiv \\{\\vec{u}_{1} \\cdots, \\vec{u}_{n}\\}, \\,\nS_{2} \\equiv \\left\\{\\sum_{i=1}^n b_{i}\\vec{u}_i\\right\\}, \\text{ and }\nS_{3} \\equiv S_{1} \\cup S_{2},\n\\end{gathered}\nbeing a member of the union of two sets means an element is within either set, or tthe intersection of both. A vector being within the span of the union of two sets means the vector is a linear combination of members of either set, or both sets. Thus we can represent the spans of S_{1} and the union $S_{1}\\cup S_{2} like\n\\begin{gathered}\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\\\\\nspan(S_{3})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n}a_{i}\\vec{u}_{i}\\right)+c\\left(\\displaystyle\\sum_{i=1}^{n}b_{i}\\vec{u}_{i}\\right)\\middle|a_{i},b_{i},c\\in\\textbf{F}\\right\\}.\n\\end{gathered}\nLike we had in the previous example we can combine both sums in the definition of span(S_{3}) by redefining a_{i}+c(b_{i})\\equiv c_{i}. Doing so reduces the two sums into one and we can see both S_{1} and S_{3} have the same span.\nThe first and second examples illustrate an important concept: the subspace spanned by a set of vectors is not unique. However, there is a crucial distinction between the two scenarios:\n\nIn the first example, we have two sets with the same number of elements that span the same subspace. This demonstrates that different sets can generate the same span.\nThe second example involves adding a new element to an existing set without changing the span. This shows that adding certain elements to a set may not expand the subspace it generates.\n\nThis distinction leads us to a fundamental property of sets of vectors known as linear independence."},"math/Tensors":{"title":"Tensors","links":["math/Vectors"],"tags":[],"content":"\nSub-Topics\nVector and Coordinate Bases"},"math/Vector_Bases":{"title":"Vector Bases and Coordinate Vectors","links":[],"tags":[],"content":"Building upon the concepts of vector spaces, linear combinations, span, and linear independence, we now delve into one of the most useful concepts in linear algebra: the vector basis.\nWe had previously defined a basis B_{W} as a linearly independent set of vectors from a space \\textbf{V} that spans a subspace \\textbf{W}. The  Moreover, we can extend this concept to define a basis for the entire vector space \\textbf{V} (there is nothing special about subspaces except for the explicit relationship to a larger vector space. We introduced bases in reference to subspaces as it flows more intuitively from span this way, but theres no reason that we couldn’t have had \\textbf{W} be an ordinary vector space and B_{W} span it).\nBy representing vectors in terms of their basis decomposition, we can express vector operations like vector addition and scalar multiplication component-wise"},"math/Vectors":{"title":"Vectors","links":["euclidean_vectors","math/What_is_a_vector"],"tags":[],"content":"\nWhy talk about vectors???\nGiven the complexity of some of the topics that will be covered in this guide, it may seem odd to start with what is likely the billionth vector review for most. While annoying for some, you can rest easy knowing this section is not intended to be a comprehensive lesson on linear-algebra, and in fact will seem blazingly quick for someone with little to no prior knowledge about vector spaces (if you have no idea what a vector is, you might find this series helpful).\nInstead, this review of some basic concepts in linear algebra serves two purposes:\n\n\n1.) Disambiguate notation that will be commonly used throughout this website and introduce readers to a more abstract formulation than the matrix algebra they may be used to\n\n\n2.) Provide a smooth transition into the discussion of Euclidean vector spaces and tensors\n\n\nFrom the second half onward, it will be assumed that you at least know what a vector is and some basic facts about vector spaces and operations on vector spaces.\nIf you feel super confident in your knowledge and feel the first section would be a waste of time, click here.\nIf you want to go ahead with the vector review, then the first question we should ask ourselves is, What is a vector?"},"math/What_is_a_vector":{"title":"What is a Vector?","links":["math/Linear_combinations"],"tags":[],"content":"The most concise answer is that a vector \\vec {u} is an element of a vector space \\textbf{V}. In order for a set \\textbf{V} to constitute a vector space, we require the existence of two operations on elements of \\textbf{V} known as vector addition and scalar multiplication. These operations must be defined in a way which is compatible with a set of requirements (called the vector axioms) that define the structure of a vector space. The vector axioms can be expressed in the following concise form:\n\n\\begin{gathered}\n\\forall \\{\\vec{u},\\vec{v},\\vec{w}\\}\\in\\textbf{V},\n\\end{gathered}\n\n\n1.)\\quad\\vec{u}+\\vec{v}\\in\\textbf{V},\\quad a\\vec{u}\\in\\textbf{V}   |   (Closure under vector addition and scalar multiplication)\n\n\n2.)\\quad\\vec{u}+(\\vec{v}+\\vec{w})=(\\vec{u}+\\vec{v})+\\vec{w}   |   (associativity of addition)\n\n\n3.)\\quad\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}   |   (commutativity of addition)\n\n\n4.)\\quad\\exists!{\\vec{0}}\\in\\textbf{V} \\,s.t. \\quad\\vec{0}+\\vec{u}=\\vec{u}   |   (existence of additive identity)\n\n\n5.)\\quad\\exists!(−{\\vec{u}})\\in\\textbf{V} \\,s.t. \\quad(−\\vec{u})+\\vec{u}=\\vec{0}   |   (existence of additive inverse)\n\n\n6.)\\quad a(b\\vec{u})=(ab)\\vec{u}   |   (scalar multiplication is equivalent to Field multiplcation)\n\n\n7.)\\quad1\\vec{u}=\\vec{u}   |   (multiplication by the identity element returns the vector original vector)\n\n\n8.)\\quad(a+b)(\\vec{u}+\\vec{v})=(a+b)\\vec{u}+(a+b)\\vec{v}=a(\\vec{u}+\\vec{v})+b(\\vec{u}+\\vec{v})=a\\vec{u}+b\\vec{u}+a\\vec{v}+b\\vec{v}   |   (scalar multiplication distributes)\n\n\n(\\forall  means for all, \\exists! means there exists only one, and \\in means within)\n\nThis definition is rather dense and includes sets not commonly thought of as vector spaces (such as the set of nth degree polynomials or even \\{0\\}). Luckily most of these requirements are things we tend to take for granted (like associativity) and so we wont have to reference them too much. We’ll also mostly be focusing on Euclidean vector spaces and that comes with a lot of geometric intuition that we can leverage.\nThe vector axioms define how to add vectors and multiply them by scalars, but a general framework for how to contruct new vectors by combining both of these operations is going to be helpful moving forward. Thus we must introduce the concept of a Linear Combination."},"math/linear_independence":{"title":"Linear Independence","links":["math/Euclidean_spaces","math/Vector_Bases"],"tags":[],"content":"A linearly independent set A\\equiv\\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} has the property that no element in A is a linear combination of any other elements (or lies within the span of the other elements). Symbolically this means\n\\begin{gathered}\n\\forall \\vec{u}_{i}\\in A,\\,\\, \\vec{u}_{i}\\notin span(A\\backslash\\{\\vec{u}_{i}\\}).\n\\end{gathered}\nUnrusprisingly, the complement to linear independence is known as linear dependence. A linearly dependent set of vectors contains elements which can be expressed as linear combinations of other elements in the set. Its important to note that no single element of a linearly dependent set is dependent, but rather they all are. This is because if we have a dependent set  B\\equiv\\{\\vec{v}_1,\\cdots,\\vec{v}_n\\} and an equation expressing any one vector in B like\n\\begin{gathered}\n\\vec{v}_{i}=\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\,\n\\end{gathered}\nthen we can express some other vector in B as a linear combinations of the other elements by first subtracting \\vec{v}_{i} from both sides to get the zero vector\n\\begin{gathered}\n\\vec{0}= \\left(\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\right) -\\vec{v}_{i}\n\\end{gathered}\ndefining c_{i}\\equiv -1 to include -\\vec{v}_i in the sum 1\n\\begin{gathered}\n\\vec{0}= \\sum_{j=1}^{n}c_{j}\\vec{v}_{j}\n\\end{gathered}\nsubtracting from both sides some c_{k}\\vec{v}_k (with c_{k}\\ne 0)\n\\begin{gathered}\n(-c_{k})\\vec{v}_k = \\sum_{j\\neq{k}}^{n} c_j \\vec{v}_j\n\\end{gathered}\nand finally multiplying both sides by -\\frac{1}{c_{k}} to get\n\\begin{gathered}\n\\vec{v}_k = \\left(-\\frac{1}{c_k}\\right)\\sum_{j \\neq k}^{n} c_j \\vec{v}_j =\\sum_{j\\neq{k}}^n \\left(-\\frac{c_{j}}{c_{k}}\\right)\\vec{v}_j\n\\end{gathered}\nwhich is an equation expressing \\vec{v}_k in terms of a linear combination of other vectors in B, making \\vec{v}_k also a dependent element.\n\nOne important fact regarding span and linear independence is that if you consider all possible spanning sets for a given subspace \\textbf{W} and focus on the ones with the smallest number of vectors, you’ll discover that each of these minimal spanning is linearly independent.\nA sketch of the proof for this comes from assuming the existence of a set S\\equiv\\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} that is both a minimal spanning set and linearly dependent.\nIf S were dependent, then we could express any one of the elements \\vec{u}_i as a linear combination of the others, as follows:\n\\begin{gathered}\n\\vec{u}_i = \\sum_{j\\neq {i}}^{n} c_{j}\\vec{v}_{j}\\,\\,.\n\\end{gathered}\nRemoving the element \\vec{u}_i from S would not alter the span of the set since any vector within span(S) that is expressed in terms of \\vec{u}_i could instead be expressed using the other vectors in the set.\nThe set S\\backslash\\{\\vec{u_{i}}\\} (which is just S excluding \\vec{u}_i) is smaller than S but has the same span, and thus S could not have been a minimal spanning set.\nSince we can repeat this process until we are left with a linearly independent set, all minimal spanning sets must be linearly independent.\nThis relationship between an independent spanning set and the space it spans brings us to one of the most important and powerful constructions in linear algebra (which will also be the last two topic we’ll cover before getting to Euclidean vector spaces), basis vectors and coordinate spaces.\nFootnotes\n\n\nAs an aside, this line provides us an alternative definition of linear dependence, which is that for any linearly dependent set \\\\B\\equiv\\{\\vec{v}_1,\\cdots,\\vec{v}_n\\},\\,\\,\\exists \\displaystyle\\sum_{i=1}^n c_{i}\\vec{v}_{i} with not all c_{i}=0 \\text{ s.t. } \\displaystyle\\sum_{i=1}^n c_{i}\\vec{v}_{i} = \\vec{0} ↩\n\n\n"}}