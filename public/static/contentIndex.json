{"index":{"title":"Burger Math","links":["math/Tensors"],"tags":[],"content":"\nTopics:\nTensors and Diff-Geo"},"math/Linear_combinations":{"title":"Linear Combinations","links":[],"tags":[],"content":"The vector axioms define the properties of scalar multiplcation and vector addition, but often we’ll seek to make use of both operations at once. This leads us to the more general concept of a linear combination.\nA linear combination of vectors is simply just a sum of vectors weighted by scalar coefficients. Given any indexable set of vectors S\\equiv \\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} within a space \\textbf{V} we can compactly represent any linear combination of vectors within this set like:\n\\begin{gathered}\n\\displaystyle\\sum_{i=1}^{n}c_{i}\\vec{u}_i=c_1\\vec{u}_1+...+c_n\\vec{u}_n\n\\end{gathered}\nWhere each coefficient could be any arbitray scalar. Also take not that since \\textbf{V} is closed under both addition and multiplication, a linear combination of vectors is itself a vector. There isn’t much added benefit that comes from discussing specific linear combinations of vectors, as they are already fully described by the vector axioms. The main benefit of defining a linear combination as we have is the ability to construct the set of all linear combinations of a given set S, which we call the span.\nFormally the span of a subset S of a vector space \\textbf{V} over a  Field {\\textbf{F}} can be defined as:\n\\begin{gathered}\nspan(S)= \\left\\{\n\\sum_{i=1}^{n}c_{i}\\vec{u}_{i} \\,\\middle|\\, c_{i}\\in\\mathbf{F},\\vec{u}_i\\in S\n\\right\\}\n\\end{gathered}\nThe span of a set of vectors is special as far as subsets of \\textbf{V} go in that it can be demonstrated to have its own vector space strucutre. All axioms dealing with the properties of operations are automatically met as a subset of a \\textbf{V}, and the remaining two axioms (existence of additive inverse and identity) can be shown to be satisfied in the following ways:\nAdditive Inverse\n\\begin{gathered}\nlet \\,\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\equiv\\vec{v}. \\\\ \\\\\\forall\\vec{v}\\in span(S),\\\\\\\\ -\\vec{v}=-\\left(\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\right) =\\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\,,\\\\\\\\ \\sum_{i=1}^n(-c_{i})\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nAdditive identity\n\\begin{gathered}\n\\vec{0}=\\sum_{i=1}^n\\vec{0}=\\sum_{i=1}^n0\\vec{u}_{i}\\, ,\\\\\\\\\\sum_{i=1}^{n}0\\vec{u}_{i}\\in span(S)\n\\end{gathered}\nThus given any set of vectors in \\textbf{V}, the span of the set is simultaneously a subset of \\textbf{V} and a   vector space of its own. All such subsets are referred to as subspaces and are generally assumed to be subsets of larger vector spaces (though all vectors spaces are technically subspaces of themselves)\nThe span of a set of vectors is not unique however. For example, take two sets in \\textbf{V},\n\\begin{gathered}\nS_{1}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_{n}\\} \\text{ and } S_{2}\\equiv\\{\\vec{u}_{1},\\cdots,\\vec{u}_n+\\vec{u}_1\\}. \\\\[1em]\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\nspan(S_{2})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n-1}c_{i}\\vec{u}_{i}\\right)+c_n\\left(\\vec{u}_{n}+\\vec{u}_{1}\\right)\\middle|c_{i}, c_{n}\\in\\textbf{F}\\right\\}\n\\end{gathered}\nWe can pretty easily see that the span of both sets should be the same, as we could redefine c_{1}\\equiv c_{1}+c_{n} (since the coefficients c_i   are arbitrary) and construct the span of both sets in exactly the same form.\nSimilarly for a triplet of sets\n\\begin{gathered}\nS_{1} \\equiv \\{\\vec{u}_{1} \\cdots, \\vec{u}_{n}\\} \\text { and }\nS_{2} \\equiv \\left\\{\\sum_{i=1}^n b_{i}\\vec{u}_i\\right\\}, \\\\\nS_{3} \\equiv S_{1} \\cup S_{2}\n\\end{gathered}\n\\begin{gathered}\nspan(S_{1})= \\left\\{\\displaystyle\\sum_{i=1}^nc_{i}\\vec{u}_{i}\\middle|c_{i}\\in\\textbf{F}\\right\\}, \\\\[1em]\\\\\nspan(S_{3})= \\left\\{\\left(\\displaystyle\\sum_{i=1}^{n}a_{i}\\vec{u}_{i}\\right)+c\\left(\\displaystyle\\sum_{j=1}^{n}b_{j}\\vec{u}_{j}\\right)\\middle|a_{i},b_{j},c\\in\\textbf{F}\\right\\}.\n\\end{gathered}\nLike we had in the previous example we can combine both sums in the definition of span(S_{3}) by using the same index and redefining a_{i}+c(b_{i})\\equiv c_{i}. Doing so we can see that once again both S_{1} and S_{3} have the same span.\nThe first and second examples illustrate an important concept: the subspace spanned by a set of vectors is not unique. However, there is a crucial distinction between the two scenarios:\n\nIn the first example, we have two sets with the same number of elements that span the same subspace. This demonstrates that different sets can generate the same span.\nThe second example involves adding a new element to an existing set without changing the span. This shows that adding certain elements to a set may not expand the subspace it generates.\n\nThis distinction leads us to a fundamental property of sets of vectors known as linear independence.\n\nLinear Independence\nA linearly independent set A\\equiv\\{\\vec{u}_1,\\cdots,\\vec{u}_n\\} has the property that no element in A is a linear combination of any other elements (or lies within the span of the other elements). Symbolically this means\n\\begin{gathered}\n\\forall \\vec{u}_{i}\\in A,\\,\\, \\vec{u}_{i}\\notin span(A\\backslash\\{\\vec{u}_{i}\\}).\n\\end{gathered}\nPredictably, the complement to linear independence is known as linear dependence. A linearly dependent set of vectors contains elements which can be expressed as linear combinations of other elements in the set. Important to note is that no single element of a linearly dependent set is dependent, but rather they all are. This is because if we have a dependent set  B\\equiv\\{\\vec{v}_1,\\cdots,\\vec{v}_n\\} and an equation expressing any one vector in B like\n\\begin{gathered}\n\\vec{v}_{i}=\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\,,\n\\end{gathered}\nthen we can express some other vector in B as a linear combinations of the other elements by first subtracting \\vec{v}_{i} from both sides to get the zero vector\n\\begin{gathered}\n\\vec{0}= \\left(\\sum_{j\\neq{i}}^{n}c_{j}\\vec{v}_{j} \\right) -\\vec{v}_{i}\n\\end{gathered}\nand then subtracting from both sides some c_{k}\\vec{v}_k (with c_{k}\\ne 0)\n\\begin{gathered}\n(-c_{k})\\vec{v}_k = \\left(\\sum_{\\substack{j\\neq{i} \\\\ j \\neq k}}^{n} c_j \\vec{v}_j\\right) - \\vec{v}_i\n\\end{gathered}\nand finally multiplying both sides by -\\frac{1}{c_{k}} to get\n\\begin{gathered}\n\\vec{v}_k = -\\frac{1}{c_k} \\left(\\left(\\sum_{\\substack{j\\neq{i} \\\\ j \\neq k}}^{n} c_j \\vec{v}_j\\right) - \\vec{v}_i\\right).\n\\end{gathered}"},"math/Tensors":{"title":"Tensors","links":["math/Vectors"],"tags":[],"content":"\nSub-Topics\nVector and Coordinate Bases"},"math/Vectors":{"title":"Vectors","links":["math/What_is_a_vector"],"tags":[],"content":"\nWhy talk about vectors???\nGiven the complexity of some of the topics that will be covered in this guide, it may seem odd to start with what is likely the billionth vector review for most. While annoying for some, you can rest easy knowing this section is not intended to be a comprehensive lesson on linear-algebra, and in fact will seem blazingly quick for someone with little to no prior knowledge bout vector spaces (if you have no idea what a vector is, you might find this series helpful).\nInstead, the first part of this page (which serves as the boiler-plate review) serves two purposes:\n\n\n1.) Disambiguate notation that will be commonly used throughout this website (this includes any future projects)\n\n\n2.) Provide a smooth transition into the latter half of this page, which will most likely be new material for some.\n\n\nFrom the second half onward, it will be assumed that you at least know what a vector is and some basic facts about vector spaces and operations on vector spaces.\nIf you feel super confident in your knowledge and feel the first section would be a waste of time, click here.\nIf you want to go ahead with the quickest vector review of all time, then the first question we should ask ourselves is, What is a vector?"},"math/What_is_a_vector":{"title":"What is a Vector?","links":["math/Linear_combinations"],"tags":[],"content":"The most concise answer is that a vector \\vec {u} is an element of a vector space \\textbf{V}. In order for a set \\textbf{V} to constitute a vector space, there is a list of requirements (called the vector axioms) that must be satisfied:\n\n\\begin{gathered}\n\\forall \\{\\vec{u},\\vec{v},\\vec{w}\\}\\in\\textbf{V},\n\\end{gathered}\n\n\n1.)\\quad\\vec{u}+\\vec{v}\\in\\textbf{V},\\quad a\\vec{u}\\in\\textbf{V}   |   (Closure under vector addition and scalar multiplication)\n\n\n2.)\\quad\\vec{u}+(\\vec{v}+\\vec{w})=(\\vec{u}+\\vec{v})+\\vec{w}   |   (associativity of addition)\n\n\n3.)\\quad\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}   |   (commutativity of addition)\n\n\n4.)\\quad\\exists!{\\vec{0}}\\in\\textbf{V} \\,s.t. \\quad\\vec{0}+\\vec{u}=\\vec{u}   |   (existence of additive identity)\n\n\n5.)\\quad\\exists!(−{\\vec{u}})\\in\\textbf{V} \\,s.t. \\quad(−\\vec{u})+\\vec{u}=\\vec{0}   |   (existence of additive inverse)\n\n\n6.)\\quad a(b\\vec{u})=(ab)\\vec{u}   |   (scalar multiplication is equivalent to Field multiplcation)\n\n\n7.)\\quad1\\vec{u}=\\vec{u}   |   (multiplication by the identity element returns the vector original vector)\n\n\n8.)\\quad(a+b)(\\vec{u}+\\vec{v})=(a+b)\\vec{u}+(a+b)\\vec{v}=a(\\vec{u}+\\vec{v})+b(\\vec{u}+\\vec{v})=a\\vec{u}+b\\vec{u}+a\\vec{v}+b\\vec{v}   |   (scalar multiplication distributes)\n\n\n(\\forall  means for all, \\exists! means there exists only one, and \\in means within)\nThis definition is rather dense and includes sets not commonly thought of as vector spaces (such as the set of nth degree polynomials or even \\{0\\}). Luckily most of these requirements are things we tend to take for granted (like associativity) and so we wont have to reference them too much. We’ll also mostly be focusing on Euclidean vector spaces and that comes with a lot of geometric intuition that we can leverage.\nThe vector axioms define how to add vectors and multiply them by scalars, but a general framework for how to contruct new vectors by combining both of these operations is a must before we can go any further. Thus we must introduce the concept of a Linear Combination."}}